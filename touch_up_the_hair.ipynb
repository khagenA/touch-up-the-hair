{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khagenA/touch-up-the-hair/blob/main/touch_up_the_hair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Zck4hO5nlJ"
      },
      "source": [
        "## Install  libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GhP5rvw6F8Q"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe\n",
        "!pip install --upgrade diffusers[torch]\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "!pip install -qU controlnet_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5O_FQFN7ZKR"
      },
      "source": [
        "# Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOissoN373vX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehA4rRXVILdo"
      },
      "source": [
        "# Create require directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sabeMdUMmlYt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "project_dir_path = '/content/drive/MyDrive/touch-up-the-hair'\n",
        "mask_dir_path = 'output_dir/masks'\n",
        "touched_dir_path = 'output_dir/touched'\n",
        "restored_dir_path = 'output_dir/touched_and_restored'\n",
        "directory_list = ['models', 'input_dir', mask_dir_path, touched_dir_path, restored_dir_path]\n",
        "for directory in directory_list:\n",
        "    directory_path = os.path.join(project_dir_path, directory)\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wIpOtJs_guz"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "selfie_multiclass_model_path = os.path.join(project_dir_path, \"models\", \"selfie_multiclass_256x256.tflite\")\n",
        "selfie_multiclass_model_url = \"https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite\"\n",
        "if(not os.path.exists(selfie_multiclass_model_path)):\n",
        "  # Download the model\n",
        "  urllib.request.urlretrieve(selfie_multiclass_model_url,selfie_multiclass_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RQZ7gDe6h6q"
      },
      "source": [
        "# Clone CodeFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWcEWIJ2xDnB"
      },
      "outputs": [],
      "source": [
        "# Clone CodeFormer and enter the CodeFormer folder\n",
        "%cd /content/\n",
        "!rm -rf CodeFormer\n",
        "!git clone https://github.com/sczhou/CodeFormer.git\n",
        "%cd CodeFormer\n",
        "\n",
        "# Set up the environment\n",
        "# Install python dependencies\n",
        "!pip install -r requirements.txt\n",
        "# Install basicsr\n",
        "!python basicsr/setup.py develop\n",
        "\n",
        "# Download the pre-trained model\n",
        "!python scripts/download_pretrained_models.py facelib\n",
        "!python scripts/download_pretrained_models.py CodeFormer\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI2NtI1yJl1h"
      },
      "outputs": [],
      "source": [
        "def face_restoration(image_in):\n",
        "    %cd /content/CodeFormer/\n",
        "    upload_folder = 'inputs/user_upload'\n",
        "    if os.path.isdir(upload_folder):\n",
        "        shutil.rmtree(upload_folder)\n",
        "    os.mkdir(upload_folder)\n",
        "    image_in.save(os.path.join(upload_folder,'image.png'))\n",
        "\n",
        "    # We set w to 0.7 for the whole images\n",
        "    # you can add '--bg_upsampler realesrgan' to enhance the background\n",
        "    #@markdown *Codeformer_Fidelity (0 for better quality, 1 for better identity)*\n",
        "    CODEFORMER_FEDILITY = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "    !python inference_codeformer.py -w $CODEFORMER_FEDILITY --input_path inputs/user_upload --bg_upsampler realesrgan\n",
        "    %cd /content/\n",
        "    img_out = Image.open(f'/content/CodeFormer/results/user_upload_{CODEFORMER_FEDILITY}/final_results/image.png')\n",
        "    return img_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxPaPvK0AKLB"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9zF4ctRzfoq"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from numpy import float32\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import torch\n",
        "import PIL\n",
        "from diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline, DPMSolverMultistepScheduler,UniPCMultistepScheduler,EulerDiscreteScheduler\n",
        "from diffusers.utils import load_image\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "import io\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0YodbHBxQDf"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Generate Class**\n",
        "class GenerateMask(object):\n",
        "\n",
        "    def __init__(self, image_path):\n",
        "\n",
        "        self._image_BGR = cv2.imread(image_path)\n",
        "        if self._image_BGR is None:\n",
        "            raise ValueError(f\"Failed to load image from path: {image_path}\")\n",
        "\n",
        "        # Convert image to RGB (OpenCV uses BGR by default)\n",
        "        self._image_RGB = cv2.cvtColor(self._image_BGR, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Model path\n",
        "        self._HAIR_SEGMENTER_MODEL_PATH = \"/content/drive/MyDrive/models/selfie_multiclass_256x256.tflite\"\n",
        "\n",
        "        # Create the options for ImageSegmenter\n",
        "        base_options = python.BaseOptions(model_asset_path = self._HAIR_SEGMENTER_MODEL_PATH)\n",
        "        self._options = vision.ImageSegmenterOptions(base_options = base_options, output_category_mask = True)\n",
        "\n",
        "        self.MASK_COLOR = (255, 255, 255)  # Define MASK_COLOR\n",
        "        self.BG_COLOR = (0, 0, 0)  # Define BG_COLOR\n",
        "\n",
        "    def get_image_BGR(self):\n",
        "        \"\"\"\n",
        "            return image_BGR\n",
        "        \"\"\"\n",
        "        return self._image_BGR\n",
        "\n",
        "    def get_image_RGB(self):\n",
        "        \"\"\"\n",
        "            return image_RGB\n",
        "        \"\"\"\n",
        "        return self._image_RGB\n",
        "\n",
        "\n",
        "    def get_mask(self,CLASS_INDEX = 1):\n",
        "        \"\"\"\n",
        "            Get the hair mask using Mediapipe's ImageSegmenter.\n",
        "        Returns:\n",
        "            - np.array: Hair mask image.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the image to Mediapipe's format\n",
        "        image_mediapipe = mp.Image(image_format = mp.ImageFormat.SRGB, data = self._image_BGR)\n",
        "\n",
        "        # Retrieve the masks for the segmented image\n",
        "        with vision.ImageSegmenter.create_from_options(self._options) as segmenter:\n",
        "            segmentation_result = segmenter.segment(image_mediapipe)\n",
        "            category_mask = segmentation_result.category_mask.numpy_view()\n",
        "\n",
        "            mask_image = np.zeros(self._image_BGR.shape, dtype = np.uint8)\n",
        "            bg_image = mask_image.copy()\n",
        "            mask_image[:] = self.MASK_COLOR\n",
        "            bg_image[:] = self.BG_COLOR\n",
        "\n",
        "            # Generate hair mask\n",
        "            condition = np.stack((category_mask,) * 3, axis = -1) == CLASS_INDEX\n",
        "            mask = np.where(condition, mask_image, bg_image)\n",
        "\n",
        "            return mask\n",
        "\n",
        "    def get_hair_mask(self):\n",
        "        return self.get_mask()\n",
        "\n",
        "    def get_face_mask(self):\n",
        "        return self.get_mask(CLASS_INDEX = 3)\n",
        "\n",
        "\n",
        "    def get_gray_mask(self,mask_path = None):\n",
        "\n",
        "        if mask_path is None:\n",
        "            # If hair_mask_path is not available, calculate hair mask on the fly\n",
        "            mask = self.get_hair_mask()\n",
        "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            # Load the binary mask\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_minority_hair_mask(self, mask_path = None, num_cluster = 2, model = \"KMeans\"):\n",
        "        \"\"\"\n",
        "        Generate a hair minority mask based on the input image and its corresponding mask.\n",
        "\n",
        "        Parameters:\n",
        "        - image_path (str): Path to the input image.\n",
        "        - mask_path (str): Path to the binary mask where hair regions are white and background is black.\n",
        "        - num_cluster (int): Number of clusters for KMeans clustering (default is 3).\n",
        "\n",
        "        Returns:\n",
        "        - np.array: Hair minority mask image.\n",
        "        \"\"\"\n",
        "\n",
        "        mask = self.get_gray_mask(mask_path = mask_path)\n",
        "\n",
        "        #Perform K-means clustering solely on hair pixels,\n",
        "        #creating two clusters instead of analyzing the entire image with three clusters.\n",
        "        #This approach is chosen to avoid potential clustering of\n",
        "        #dark black hair with the background due to similar color tones.\n",
        "        #Subsequently, identify non-zero pixels within the mask, representing the masked area.\n",
        "        mask_indices = np.where(mask > 0)\n",
        "\n",
        "        # Get the pixels within the masked area\n",
        "        pixels = self._image_RGB[mask_indices].reshape((-1, 3))\n",
        "\n",
        "        # Perform KMeans clustering on the pixels\n",
        "        if model == \"KMeans\":\n",
        "          model = KMeans(n_clusters = num_cluster, n_init = 10)\n",
        "        elif model == \"GaussianMixture\":\n",
        "          model = GaussianMixture(n_components = num_cluster, covariance_type = 'full')\n",
        "        else:\n",
        "          raise ValueError(\"Invalid model name. Valid options are 'KMeans' and 'GaussianMixture'.\")\n",
        "\n",
        "        model.fit(pixels)\n",
        "\n",
        "        # Get the labels for each pixel\n",
        "        labels = model.predict(pixels)\n",
        "\n",
        "        # Find the cluster with the fewest pixels\n",
        "        min_cluster_label = np.bincount(labels).argmin()\n",
        "\n",
        "        # Create a binary mask where pixels belonging to the minimum cluster class are white\n",
        "        cluster_mask = np.zeros_like(mask)\n",
        "        cluster_mask[mask_indices] = 255 * (labels == min_cluster_label)\n",
        "\n",
        "        rgb_img_hair_minority_mask = cv2.cvtColor(cluster_mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        return rgb_img_hair_minority_mask\n",
        "\n",
        "\n",
        "    def get_gray_minority_mask(self, minority_mask_path = None):\n",
        "        if minority_mask_path is None:\n",
        "            # If hair_mask_path is not available, calculate hair mask on the fly\n",
        "            minority_mask = self.get_minority_hair_mask()\n",
        "            minority_mask = cv2.cvtColor(minority_mask, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            # Load the binary mask\n",
        "            minority_mask = cv2.imread(minority_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        return minority_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgSaZvgv-rZt"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Stable Diffusion Controlnet Pipeline Class**\n",
        "class StableDiffusionControlnetPipeline(object):\n",
        "    def __init__(self):\n",
        "        self.MODEL_PATHS = {\n",
        "            \"RealVisInpaint\": \"Uminosachi/realisticVisionV51_v51VAE-inpainting\"\n",
        "        }\n",
        "        self.CONTROLNET_PATHS = {\n",
        "            \"ControlNetInpaint\": \"lllyasviel/control_v11p_sd15_inpaint\",\n",
        "            \"ControlNetCanny\": \"lllyasviel/control_v11p_sd15_canny\",\n",
        "        }\n",
        "\n",
        "        self.schedulers = {\n",
        "            \"UniPCMultistepScheduler\": UniPCMultistepScheduler,\n",
        "            \"DPMSolverMultistepScheduler\": DPMSolverMultistepScheduler,\n",
        "            \"EulerDiscreteScheduler\": EulerDiscreteScheduler\n",
        "        }\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        #Loading the ControlNet Inpaint and Canny Models\n",
        "        self.controlnet = [\n",
        "            ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=torch.float16).to(self.device) for _, CONTROLNET_PATH in self.CONTROLNET_PATHS.items()\n",
        "        ]\n",
        "        controlnet_inpaint_scale = 0.5 # @param {type:\"number\"}\n",
        "        controlnet_canny_scale = 0.2 # @param {type:\"number\"}\n",
        "        self.controlnet_scales = [controlnet_inpaint_scale, controlnet_canny_scale]\n",
        "\n",
        "\n",
        "        assert len(self.controlnet) == len(self.controlnet_scales)\n",
        "\n",
        "        #Loading the Stabel Difussion RealisticVision ControlNet Inpaint Pipeline\n",
        "        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
        "            self.MODEL_PATHS[\"RealVisInpaint\"],\n",
        "            controlnet=self.controlnet,\n",
        "            safety_checker=None,\n",
        "            requires_safety_checker=False,\n",
        "            torch_dtype=torch.float16,\n",
        "        ).to(self.device)\n",
        "        pipe.scheduler = self.schedulers[\"UniPCMultistepScheduler\"].from_config(pipe.scheduler.config)\n",
        "        self.pipe = pipe\n",
        "\n",
        "    def make_inpaint_condition(self, image, image_mask):\n",
        "        image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
        "        image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n",
        "        assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n",
        "        image[image_mask > 0.5] = -1.0  # set as masked pixel\n",
        "        image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n",
        "        image = torch.from_numpy(image)\n",
        "        return image\n",
        "\n",
        "    def make_canny_condition(self, image):\n",
        "        image = np.array(image)\n",
        "        image = cv2.Canny(image, 100, 200)\n",
        "        image = image[:, :, None]\n",
        "        image = np.concatenate([image, image, image], axis=2)\n",
        "        image = Image.fromarray(image)\n",
        "        return image\n",
        "\n",
        "    def roundUp(self, input, round):\n",
        "        return input + round - (input % round)\n",
        "\n",
        "    # Function to Edit the Hair Roots of the Img with Stable Diffusion and ControlNet\n",
        "    def stable_diffusion_controlnet(self, image_path, PROMPT, NEGATIVE_PROMPT, minority_mask_path = None):\n",
        "        pillow_img = Image.open(image_path)\n",
        "        cv2_img = np.asarray(pillow_img)\n",
        "        if minority_mask_path is None:\n",
        "          # If hair_mask_path is not available, calculate hair mask on the fly\n",
        "          mask_generater = GenerateMask(image_path)\n",
        "          HAIR_ROOT_MASK = mask_generater.get_minority_hair_mask()\n",
        "        else:\n",
        "          # Load the binary mask\n",
        "            HAIR_ROOT_MASK = cv2.imread(minority_mask_path)\n",
        "\n",
        "        HAIR_ROOT_MASK = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        kernel = np.ones((12, 12), np.uint8)#expansion gives better results\n",
        "        HAIR_ROOT_MASK = cv2.dilate(HAIR_ROOT_MASK, kernel, iterations=1)\n",
        "\n",
        "\n",
        "        mask_image = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2RGB)\n",
        "        mask_image = Image.fromarray(mask_image)\n",
        "\n",
        "        # Resize the image to be divisible by 8\n",
        "        height = self.roundUp(pillow_img.height, 8)\n",
        "        width = self.roundUp(pillow_img.width, 8)\n",
        "        SEED = 42 # @param {type:\"integer\"}\n",
        "        num_inference_steps = 50 # @param {type:\"integer\"},\n",
        "        strength = 1.0 # @param {type:\"number\"},\n",
        "        guidance_scale = 4.0 # @param {type:\"number\"}\n",
        "\n",
        "        generator = torch.Generator(device=self.device).manual_seed(SEED)\n",
        "\n",
        "        inpaint_condition = self.make_inpaint_condition(pillow_img, mask_image)\n",
        "        canny_condition = self.make_canny_condition(pillow_img)\n",
        "        control_images = [inpaint_condition, canny_condition]\n",
        "        touched_image = self.pipe(\n",
        "            prompt = PROMPT,\n",
        "            image = pillow_img,\n",
        "            mask_image = mask_image,\n",
        "            num_inference_steps = num_inference_steps,\n",
        "            generator = generator,\n",
        "            control_image=control_images,\n",
        "            controlnet_conditioning_scale = self.controlnet_scales,\n",
        "            negative_prompt = NEGATIVE_PROMPT,\n",
        "            strength = strength,\n",
        "            height = height,\n",
        "            width = width,\n",
        "            guidance_scale = guidance_scale,\n",
        "\n",
        "        ).images[0]\n",
        "\n",
        "\n",
        "        return HAIR_ROOT_MASK, touched_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrn3lBm_ntnb"
      },
      "source": [
        "# Instantiate the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0d6-_2qzTKW"
      },
      "outputs": [],
      "source": [
        "SB_ControlNet_pipeline = StableDiffusionControlnetPipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoLKx9z_5547"
      },
      "source": [
        "# **Prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIPnazdo3vGx"
      },
      "outputs": [],
      "source": [
        "def merge_prompts(prompts):\n",
        "  return \", \".join(prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rPBFkHzh3j3"
      },
      "outputs": [],
      "source": [
        "# @markdown #**Positive Prompt**\n",
        "\n",
        "PROMPT_1 = \"(change the color of the hair roots to be like the rest of the hair color:1.2)\" # @param {type:\"string\"}\n",
        "PROMPT_2 = \"raw photo, high detail, best quality, keep the same style of the hair\" # @param {type:\"string\"}\n",
        "PROMPT_3 = \"beautiful hair, beautiful hair roots\" # @param {type:\"string\"}\n",
        "prompt_list = [PROMPT_1, PROMPT_2, PROMPT_3]\n",
        "PROMPT = merge_prompts(prompt_list)\n",
        "# @markdown #**Negative Prompt**\n",
        "\n",
        "NEG_PROMPT_1 =  \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime)\" # @param {type:\"string\"}\n",
        "NEG_PROMPT_2 = \"text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers\" # @param {type:\"string\"}\n",
        "NEG_PROMPT_3 = \"mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions\" # @param {type:\"string\"}\n",
        "NEG_PROMPT_4 = \"extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs\"# @param {type:\"string\"}\n",
        "NEG_PROMPT_5 = \"fused fingers, too many fingers, long neck\" # @param {type:\"string\"}\n",
        "negative_prompt_list = [NEG_PROMPT_1, NEG_PROMPT_2, NEG_PROMPT_3, NEG_PROMPT_4, NEG_PROMPT_5]\n",
        "NEGATIVE_PROMPT = merge_prompts(negative_prompt_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B5l3a30G-M1"
      },
      "source": [
        "# **Touch up the hair for a given list of images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxZ93Q0JXPjZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "demo_img_list = [1,19,21,22,32]\n",
        "if __name__ == \"__main__\":\n",
        "  for i in tqdm(demo_img_list):\n",
        "    IMG_PATH =  os.path.join(project_dir_path,\"Images\", \"pic\" + str(i) + \".png\")\n",
        "    MINORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, mask_dir_path,\"pic\" + str(i) + \"_mask.png\")\n",
        "    TOUCHED_IMAGE_PATH = os.path.join(project_dir_path, touched_dir_path, \"pic\" + str(i) + \"_touched.png\")\n",
        "    RESTORED_IMAGE_PATH = os.path.join(project_dir_path, restored_dir_path, \"pic\" + str(i) + \"_restored.png\")\n",
        "    # Process the image with the pipeline\n",
        "    minority_hair_mask, touched_image = SB_ControlNet_pipeline.stable_diffusion_controlnet(IMG_PATH, PROMPT, NEGATIVE_PROMPT)\n",
        "    touched_image_restored = face_restoration(touched_image)\n",
        "    # Save the results\n",
        "    cv2.imwrite(MINORITY_HAIR_MASK_PATH, minority_hair_mask)\n",
        "    touched_image.save(TOUCHED_IMAGE_PATH)\n",
        "    touched_image_restored.save(RESTORED_IMAGE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGBT5Jo0L6hD"
      },
      "outputs": [],
      "source": [
        "def apply_face_mask_overlay(image, face_mask, color = (0, 255, 255), alpha = 1.0):\n",
        "    pil_image = image.convert('RGB')\n",
        "    open_cv_image = np.array(pil_image)\n",
        "    # Convert RGB to BGR\n",
        "    input_image = open_cv_image[:, :, ::-1].copy()\n",
        "    # Load the input image and mask\n",
        "    gray_mask = cv2.cvtColor(face_mask, cv2.COLOR_BGR2GRAY)\n",
        "    mask_image = cv2.resize(gray_mask, input_image.shape[:2])\n",
        "\n",
        "    # Create a blank image with the same dimensions as the input image\n",
        "    overlay = np.zeros_like(input_image)\n",
        "\n",
        "    # Apply the overlay color to the blank image using the mask\n",
        "    overlay[mask_image > 0] = color\n",
        "\n",
        "    result = input_image.copy()\n",
        "    # Combine the input image and the overlay using alpha blending\n",
        "    result[mask_image > 0] = cv2.addWeighted(input_image[mask_image > 0], 0.1, overlay[mask_image > 0], alpha, 0)\n",
        "\n",
        "\n",
        "    # Convert the resulting image from OpenCV to PIL format\n",
        "    result_pil = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    return result_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B9TmLFJuNQ0P"
      },
      "outputs": [],
      "source": [
        "demo_img_list = [1,19,21,22,32]\n",
        "ReSize = (512,512)\n",
        "img_titles = [\"Original Image\", \"Touched Image\", \"Restored Image\"]\n",
        "if __name__ == \"__main__\":\n",
        "  for n, i in enumerate(demo_img_list):\n",
        "    fig, axes = plt.subplots(1, 3, figsize = (16,9))\n",
        "    IMG_PATH =  os.path.join(project_dir_path,\"Images\", \"pic\" + str(i) + \".png\")\n",
        "    MINORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, mask_dir_path,\"pic\" + str(i) + \"_mask.png\")\n",
        "    TOUCHED_IMAGE_PATH = os.path.join(project_dir_path, touched_dir_path, \"pic\" + str(i) + \"_touched.png\")\n",
        "    RESTORED_IMAGE_PATH = os.path.join(project_dir_path, restored_dir_path, \"pic\" + str(i) + \"_restored.png\")\n",
        "    mask_generater = GenerateMask(IMG_PATH)\n",
        "    face_mask = mask_generater.get_face_mask()\n",
        "    image = Image.open(IMG_PATH)\n",
        "    touched_image = Image.open(TOUCHED_IMAGE_PATH)\n",
        "    restored_image = Image.open(RESTORED_IMAGE_PATH)\n",
        "    img_list = [image.resize(ReSize), touched_image.resize(ReSize), restored_image.resize(ReSize)]\n",
        "    for j, img_ in enumerate(img_list):\n",
        "        img = apply_face_mask_overlay(img_,face_mask)\n",
        "\n",
        "        axes[j].imshow(img)\n",
        "        axes[j].axis('off')\n",
        "        if n == 0:\n",
        "          axes[j].set_title(img_titles[j])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}